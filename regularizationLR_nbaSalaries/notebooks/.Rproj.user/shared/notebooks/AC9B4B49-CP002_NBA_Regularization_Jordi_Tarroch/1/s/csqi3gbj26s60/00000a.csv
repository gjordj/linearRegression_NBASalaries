"0","############################"
"0","# TUNING ELASTIC NETS λ y α###################################################################################"
"0","############################"
"0","# We try a bunch of different values for alpha to see a bunch of Elastic-Net Regression fits."
"0","# INPUT: We need nba_train_x from the feature matrix and nba_train_y to tune it."
"0","# OUTPUT: MSE_min, MSE_1se for each lambda_min, lambda_1se and alpha tried."
"0",""
"0",""
"0",""
"0","# maintain the same folds across all models"
"0","fold_id <- sample(1:10, size = length(nba_train_y), replace=TRUE)"
"0",""
"0","# search across a range of alphas"
"0","tuning_grid <- tibble::tibble("
"0","  alpha      = seq(0, 1, by = .1),"
"0","  "
"0","  mse_min    = NA,"
"0","  mse_1se    = NA,"
"0","  lambda_min = NA,"
"0","  lambda_1se = NA"
"0",")"
"0",""
"0",""
"0","for(i in seq_along(tuning_grid$alpha)) {"
"0","  # cv.glmnet: we want to use cross-validation to obtain the optimal values for lambda. By default it uses 10-Fold cross-validation. For that we need to specify the Training sets: The train_x to predict train_y."
"0",""
"0","  # fit CV model for each alpha value"
"0","  # PARAMETERS: Training sets: The train_x to predict train_y."
"0","  fit <- cv.glmnet(nba_train_x, nba_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)"
"0","  "
"0","  # MSE: sum of the squared residuals divided by the sample size."
"0","  # extract MSE and lambda values."
"0","  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]"
"0","  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]"
"0","  tuning_grid$lambda_min[i] <- fit$lambda.min"
"0","  tuning_grid$lambda_1se[i] <- fit$lambda.1se"
"0","}"
"0",""
"0","tuning_grid"
