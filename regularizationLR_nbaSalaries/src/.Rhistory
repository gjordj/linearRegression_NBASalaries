# 4 MODELS CREATED#############################################################################
library(MASS)
regfit.bwd=regsubsets(Salary~ NBA_DraftNumber+Age+Tm+G+MP+PER+TS+PAr+FTr+
ORB+DRB+TRB+AST+STL+BLK+TOV+USG+OWS+DWS+
WS+WSM+OBPM+DBPM+BPM+VORP,mData,method ="backward")
IC<-summary (regfit.bwd )
for(i in 1:length(IC$bic)){
if(IC$bic[i] == sort(IC$bic)[1]){
first_best = i
}
if(IC$bic[i] == sort(IC$bic)[2]){
second_best = i
}
if(IC$bic[i] == sort(IC$bic)[3]){
third_best = i
}
if(IC$bic[i] == sort(IC$bic)[4]){
fourth_best = i
}
}
backward_stepwise_ic<-IC$bic[first_best]
print(IC$bic[first_best])
print(IC$bic[second_best])
print(IC$bic[third_best])
print(IC$bic[fourth_best])
#########
# PSEUDO#
#########
# Get variable names used in a model.
# Get first 4 models with best BIC, its variable names used in the model.
# Get variable names used in best 4 models based on BIC.
# Create the 4 best models based on BIC.
model<-as.data.frame(IC$which[first_best,])
names(model)[1]<-"used"
parameters_model_first<-c()
for(i in 1:dim(model)[1]){
if(model$used[i] == TRUE){
parameters_model_first<- parameters_model_first%>%append(row.names(model)[i])
}
}
model<-as.data.frame(IC$which[second_best,])
names(model)[1]<-"used"
parameters_model_second<-c()
for(i in 1:dim(model)[1]){
if(model$used[i] == TRUE){
parameters_model_second<- parameters_model_second%>%append(row.names(model)[i])
}
}
model<-as.data.frame(IC$which[third_best,])
names(model)[1]<-"used"
parameters_model_third<-c()
for(i in 1:dim(model)[1]){
if(model$used[i] == TRUE){
parameters_model_third<- parameters_model_third%>%append(row.names(model)[i])
}
}
model<-as.data.frame(IC$which[fourth_best,])
names(model)[1]<-"used"
parameters_model_fourth<-c()
for(i in 1:dim(model)[1]){
if(model$used[i] == TRUE){
parameters_model_fourth<- parameters_model_fourth%>%append(row.names(model)[i])
}
}
# Eliminamos variable Intercept de la muestra.
parameters_model_first<-parameters_model_first[-1]
parameters_model_second<-parameters_model_second[-1]
parameters_model_third<-parameters_model_third[-1]
parameters_model_fourth<-parameters_model_fourth[-1]
for(i in 1:length(parameters_model_first)){
if(parameters_model_first[i] == "TmHOU"){
parameters_model_first[i]="HOU"
}
}
for(i in 1:length(parameters_model_second)){
if(parameters_model_second[i] == "TmHOU"){
parameters_model_second[i]="HOU"
}
}
for(i in 1:length(parameters_model_third)){
if(parameters_model_third[i] == "TmHOU"){
parameters_model_third[i]="HOU"
}
}
for(i in 1:length(parameters_model_fourth)){
if(parameters_model_fourth[i] == "TmHOU"){
parameters_model_fourth[i]="HOU"
}
}
#
# # Correct TmHou
# for (i in 1:length(parameters_model_first)){
#   if("TmHOU" %in% parameters_model_first[i]){
#     parameters_model_first[i]<-"Tm"}
# }
#
# for (i in 1:length(parameters_model_second)){
#   if("TmHOU" %in% parameters_model_second[i]){
#     parameters_model_second[i]<-"Tm"}
# }
#
# for (i in 1:length(parameters_model_third)){
#   if("TmHOU" %in% parameters_model_third[i]){
#     parameters_model_third[i]<-"Tm"}
# }
#
# for (i in 1:length(parameters_model_fourth)){
#   if("TmHOU" %in% parameters_model_fourth[i]){
#     parameters_model_fourth[i]<-"Tm"}
# }
# parameters_model_first
# parameters_model_second
# parameters_model_third
# parameters_model_fourth
parameters_model_first_backward_stepwise<-parameters_model_first
parameters_model_second_backward_stepwise<-parameters_model_second
parameters_model_third_backward_stepwise<-parameters_model_third
parameters_model_fourth_backward_stepwise<-parameters_model_fourth
##################################################################################
# CREATION OF THE BEST 4 MODELS OUT OF THE 8 WITH BEST BIC AUTOMATICALLY SELECTED#
##################################################################################
#Create first best model
check <-mData
check<-check%>%mutate(HOU=0)
for(i in 1:dim(mData)[1]){
if(mData$Tm[i] == "HOU"){
check$HOU[i] = 1
}
else{check$HOU[i] = 0
}
}
erase_columns<-c()
for(j in 1:dim(check)[2]){
c<-0
for( i in 1:length(parameters_model_first)){
if(names(check)[j] == parameters_model_first[i]){
c<-c+1
}
}
if(c == 0){erase_columns<-c(erase_columns, j)}
}
erase_columns<-erase_columns[-1:-3]
model1 <- check[ -erase_columns ]
#Create second best model
check <-mData
check<-check%>%mutate(HOU=0)
for(i in 1:dim(mData)[1]){
if(mData$Tm[i] == "HOU"){
check$HOU[i] = 1
}
else{check$HOU[i] = 0
}
}
erase_columns<-c()
for(j in 1:dim(check)[2]){
c<-0
for( i in 1:length(parameters_model_second)){
if(names(check)[j] == parameters_model_second[i]){
c<-c+1
}
}
if(c == 0){erase_columns<-c(erase_columns, j)}
}
erase_columns<-erase_columns[-1:-3]
model2 <- check[ -erase_columns ]
#Create third best model
check <-mData
check<-check%>%mutate(HOU=0)
for(i in 1:dim(mData)[1]){
if(mData$Tm[i] == "HOU"){
check$HOU[i] = 1
}
else{check$HOU[i] = 0
}
}
erase_columns<-c()
for(j in 1:dim(check)[2]){
c<-0
for( i in 1:length(parameters_model_third)){
if(names(check)[j] == parameters_model_third[i]){
c<-c+1
}
}
if(c == 0){erase_columns<-c(erase_columns, j)}
}
erase_columns<-erase_columns[-1:-3]
model3 <- check[ -erase_columns ]
#Create fourth best model
check <-mData
check<-check%>%mutate(HOU=0)
for(i in 1:dim(mData)[1]){
if(mData$Tm[i] == "HOU"){
check$HOU[i] = 1
}
else{check$HOU[i] = 0
}
}
erase_columns<-c()
for(j in 1:dim(check)[2]){
c<-0
for( i in 1:length(parameters_model_fourth)){
if(names(check)[j] == parameters_model_fourth[i]){
c<-c+1
}
}
if(c == 0){erase_columns<-c(erase_columns, j)}
}
erase_columns<-erase_columns[-1:-3]
model4 <- check[ -erase_columns ]
# 4 MODELS CREATED#############################################################################
# parameters_model_first
# parameters_model_second
# parameters_model_third
# parameters_model_fourth
##############################
# CHECK BEST METHOD SELECTION#
##############################
cat("Best Subset IC", best_subset_ic, "\n")
cat("Forward Stepwise IC", forward_stepwise_ic, "\n")
cat("Backward Stepwise IC", backward_stepwise_ic, "\n")
cat("The method Selection with best IC (lowest IC) are Best Subset and Backward Stepwise")
######################
#CARRY OUT REGRESSION#
######################
# names(model1)
# names(model2)
# names(model3)
# names(model4)
parameters_model_first_backward_stepwise
parameters_model_second_backward_stepwise
parameters_model_third_backward_stepwise
parameters_model_fourth_backward_stepwise
regres01=lm(Salary~ NBA_DraftNumber+Age+G+MP+WS,data=check)
regres02=lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP,data=check)
regres03=lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,data=check)
regres04=lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,data=check)
# Assessing outliers
outlierTest(regres01)
outlierTest(regres02)
outlierTest(regres03)
outlierTest(regres04)
#  Identifying high leverage points
hat.plot <- function(fit) {
p <- length(coefficients(fit))
n <- length(fitted(fit))
plot(hatvalues(fit), main="Index Plot of Hat Values")
abline(h=c(2,3)*p/n, col="red", lty=2)
identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}
hat.plot(regres01)
# Identifying influential observations
# Cooks Distance D
# identify D values > 4/(n-k-1)
cutoff <- 4/(nrow(check)-length(regres01$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)+
abline(h=cutoff, lty=2, col="red")
# Cooks Distance D
# identify D values > 4/(n-k-1)
cutoff <- 4/(nrow(check)-length(regres02$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)+
abline(h=cutoff, lty=2, col="red")
# Cooks Distance D
# identify D values > 4/(n-k-1)
cutoff <- 4/(nrow(check)-length(regres03$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)+
abline(h=cutoff, lty=2, col="red")
# Cooks Distance D
# identify D values > 4/(n-k-1)
cutoff <- 4/(nrow(check)-length(regres04$coefficients)-2)
plot(regres01, which=4, cook.levels=cutoff)+
abline(h=cutoff, lty=2, col="red")
# Influence Plot
influencePlot(regres01, main="Influence Plot 1",
sub="Circle size is proportional to Cook's Distance" )
# Influence Plot
influencePlot(regres02, main="Influence Plot 2",
sub="Circle size is proportional to Cook's Distance" )
# Influence Plot
influencePlot(regres03, main="Influence Plot 3",
sub="Circle size is proportional to Cook's Distance" )
# Influence Plot
influencePlot(regres04, main="Influence Plot 4",
sub="Circle size is proportional to Cook's Distance" )
# StudRes
# <dbl>
# Hat
# <dbl>
# CookD
# <dbl>
# 114	4.2632261	0.009536052	0.028155028
# 194	1.0614194	0.046872313	0.009231523
# 229	-3.0758790	0.045776304	0.074331584
# 296	0.4743764	0.070640588	0.002855412
# 328	4.4759462	0.016250905	0.053050334
# Added variable plots
# add id.method="identify" to interactively identify points
avPlots(regres01, ask=FALSE)
# Added variable plots
# add id.method="identify" to interactively identify points
avPlots(regres02, ask=FALSE)
# Added variable plots
# add id.method="identify" to interactively identify points
avPlots(regres03, ask=FALSE)
# Added variable plots
# add id.method="identify" to interactively identify points
avPlots(regres04, ask=FALSE)
# StudRes
# <dbl>
# Hat
# <dbl>
# CookD
# <dbl>
# 114	4.2632261	0.009536052	0.028155028
# 194	1.0614194	0.046872313	0.009231523
# 229	-3.0758790	0.045776304	0.074331584
# 296	0.4743764	0.070640588	0.002855412
# 328	4.4759462	0.016250905	0.053050334
filtered1<-check
erase_rows<-c(70,114, 229, 194, 296,328)
filtered1 <- filtered1[-erase_rows,]
library(ISLR)
library(gvlma)
set.seed(250)
numData=nrow(filtered1)
train=sample(numData ,numData/2)
parameters_model_first_backward_stepwise
regres.train =lm(Salary~ NBA_DraftNumber+Age+G+MP+WS,filtered1 ,subset =train )
attach(filtered1)
mean((Salary-predict(regres.train ,Auto))[-train ]^2)
set.seed(251)
regres.train2 =lm(Salary~ NBA_DraftNumber+Age+G+MP+WS,filtered1 ,subset =train )
r<-mean((Salary-predict(regres.train2 ,Auto))[-train ]^2)
r
error_model1<-sqrt(r)
cat("We are going to be wrong in our prediction around", sqrt(r), "$")
library (boot)
parameters_model_first_backward_stepwise
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+G+MP+WS,filtered1,family = gaussian())
coef(glm.fit1)
cv.err =cv.glm(filtered1,glm.fit1)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered1,family = gaussian())
cv.err2 =cv.glm(filtered1,glm.fit2)
cv.err2$delta
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered1,family = gaussian())
cv.err =cv.glm(filtered1,glm.fit1,K=10)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered1,family = gaussian())
cv.err2 =cv.glm(filtered1,glm.fit2,K=10)
cv.err2$delta
# StudRes
# <dbl>
# Hat
# <dbl>
# CookD
# <dbl>
# 114	4.2559677	0.08078916	0.04381844
# 194	1.6428464	0.11669390	0.01014907
# 328	4.0827552	0.08211991	0.04117527
# 429	0.8959193	0.11911435	0.00310245
filtered2<-check
erase_rows<-c(70, 229,114, 194, 429,328)
filtered2 <- filtered2[-erase_rows,]
library(ISLR)
library(gvlma)
set.seed(250)
numData=nrow(filtered2)
train=sample(numData ,numData/2)
parameters_model_second_backward_stepwise
regres.train =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2 ,subset =train )
attach(filtered2)
mean((Salary-predict(regres.train ,Auto))[-train ]^2)
set.seed(251)
regres.train2 =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2 ,subset =train )
r<-mean((Salary-predict(regres.train2 ,Auto))[-train ]^2)
r
error_model2<-sqrt(r)
cat("We are going to be wrong in our prediction around", sqrt(r), "$")
library (boot)
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2,family = gaussian())
coef(glm.fit1)
cv.err =cv.glm(filtered2,glm.fit1)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2,family = gaussian())
cv.err2 =cv.glm(filtered2,glm.fit2)
cv.err2$delta
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2,family = gaussian())
cv.err =cv.glm(filtered2,glm.fit1,K=10)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+WS,filtered2,family = gaussian())
cv.err2 =cv.glm(filtered2,glm.fit2,K=10)
cv.err2$delta
# 114	4.3845261	0.08331620	0.045377111
# 194	0.7428795	0.14579735	0.002548346
# 296	1.2936442	0.15196862	0.008093150
# 328	4.4133870	0.08329937	0.045941308
filtered3<-check
erase_rows<-c(70, 229, 114, 194, 296,328)
filtered3 <- filtered3[-erase_rows,]
library(ISLR)
library(gvlma)
set.seed(250)
numData=nrow(filtered3)
train=sample(numData ,numData/2)
parameters_model_third_backward_stepwise
regres.train =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3 ,subset =train )
attach(filtered3)
mean((Salary-predict(regres.train ,Auto))[-train ]^2)
set.seed(251)
regres.train2 =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3 ,subset =train )
r<-mean((Salary-predict(regres.train2 ,Auto))[-train ]^2)
r
error_model3<-sqrt(r)
cat("We are going to be wrong in our prediction around", sqrt(r), "$")
library (boot)
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3,family = gaussian())
coef(glm.fit1)
cv.err =cv.glm(filtered3,glm.fit1)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3,family = gaussian())
cv.err2 =cv.glm(filtered3,glm.fit2)
cv.err2$delta
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3,family = gaussian())
cv.err =cv.glm(filtered3,glm.fit1,K=10)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+TRB+WS,filtered3,family = gaussian())
cv.err2 =cv.glm(filtered3,glm.fit2,K=10)
cv.err2$delta
# StudRes
# <dbl>
# Hat
# <dbl>
# CookD
# <dbl>
# 114	4.3576869	0.08372868	0.0438978287
# 225	0.1775986	0.24637139	0.0002719384
# 240	-0.3621925	0.15669959	0.0006427260
# 328	4.2576119	0.09531112	0.0484020015
filtered4<-check
erase_rows<-c(70, 229, 114, 225, 240,328)
filtered4 <- filtered4[-erase_rows,]
library(ISLR)
library(gvlma)
set.seed(250)
numData=nrow(filtered4)
train=sample(numData ,numData/2)
parameters_model_fourth_backward_stepwise
regres.train =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4 ,subset =train )
attach(filtered4)
mean((Salary-predict(regres.train ,Auto))[-train ]^2)
set.seed(251)
regres.train2 =lm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4 ,subset =train )
r<-mean((Salary-predict(regres.train2 ,Auto))[-train ]^2)
r
error_model4<-sqrt(r)
cat("We are going to be wrong in our prediction around", sqrt(r), "$")
library (boot)
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4,family = gaussian())
coef(glm.fit1)
cv.err =cv.glm(filtered4,glm.fit1)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4,family = gaussian())
cv.err2 =cv.glm(filtered4,glm.fit2)
cv.err2$delta
glm.fit1=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4,family = gaussian())
cv.err =cv.glm(filtered4,glm.fit1,K=10)
cv.err$delta
glm.fit2=glm(Salary~ NBA_DraftNumber+Age+HOU+G+MP+ORB+TRB+WS,filtered4,family = gaussian())
cv.err2 =cv.glm(filtered4,glm.fit2,K=10)
cv.err2$delta
cat(" After choosing the best 4 models from one of these automated selection methods \n (best subset, forward stepwise, backward stepwise), we've filtered the data for each model and we've calculated \n the MSE or also the Error in Millions (if we square root the MSE) for each one.\n
We have ordered each model based on its BIC, which penalizes the amount of variables, but after calculating\n its MSE, we can see that lowest BIC doesn't mean lowest MSE. \n These are the results of the error in Millions for each model ordered by lowest BIC:
Model 1:", error_model1,
"Model 2:", error_model2,
"Model 3:", error_model3,
"Model 4:", error_model4, "\n
\n The model with less prediction error is the one with these parameters.:\n
", parameters_model_fourth_backward_stepwise,"\n
Lowet BIC doesn't mean lowest MSE. In fact BIC penalizes adding variables."
)
# Plots empirical quantiles of a variable, or of studentized residuals from a linear model, against theoretical quantiles of a comparison distribution.
qqPlot(regres04, labels=row.names(filtered1), id.method="identify",
simulate=TRUE, main="Q-Q Plot")
residplot <- function(fit, nbreaks=10) {
z <- rstudent(fit)
hist(z, breaks=nbreaks, freq=FALSE,
xlab="Studentized Residual",
main="Distribution of Errors")
rug(jitter(z), col="brown")
curve(dnorm(x, mean=mean(z), sd=sd(z)),
add=TRUE, col="blue", lwd=2)
lines(density(z)$x, density(z)$y,
col="red", lwd=2, lty=2)
legend("topright",
legend = c( "Normal Curve", "Kernel Density Curve"),
lty=1:2, col=c("blue","red"), cex=.7)
}
residplot(regres04)
vResid=resid(regres04)
library(fBasics)
jbTest(vResid)
#We see in this Jarque - Bera Normality Test that the distribution is not Normal because the p-value is <<<<5%
shapiro.test(vResid)##The distribution is not Normal because the p-value is <<<<5%
crPlots(regres04)
ncvTest(regres04)
spreadLevelPlot(regres04)
library(gvlma)
gvmodel <- gvlma(regres04)
summary(gvmodel)
vif(regres04)
sqrt(vif(regres04)) > 2 # problem?
View(mData)
